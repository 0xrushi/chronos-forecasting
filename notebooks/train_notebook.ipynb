{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e3783a-8006-4da6-8709-a2de47ccc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/amazon-science/chronos-forecasting.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aaeab1e-7091-4b64-9cc1-3e1bf426cb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bread/opt/anaconda3/envs/mlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "# import chronos\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GenerationConfig,\n",
    "    PreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b0896e-1f69-45cb-8721-93228bf25d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChronosConfig:\n",
    "    \"\"\"\n",
    "    This class holds all the configuration parameters to be used\n",
    "    by ``ChronosTokenizer`` and ``ChronosModel``.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer_class: str\n",
    "    tokenizer_kwargs: Dict[str, Any]\n",
    "    n_tokens: int\n",
    "    n_special_tokens: int\n",
    "    pad_token_id: int\n",
    "    eos_token_id: int\n",
    "    use_eos_token: bool\n",
    "    model_type: Literal[\"causal\", \"seq2seq\"]\n",
    "    context_length: int\n",
    "    prediction_length: int\n",
    "    num_samples: int\n",
    "    temperature: float\n",
    "    top_k: int\n",
    "    top_p: float\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (\n",
    "            self.pad_token_id < self.n_special_tokens\n",
    "            and self.eos_token_id < self.n_special_tokens\n",
    "        ), f\"Special token id's must be smaller than {self.n_special_tokens=}\"\n",
    "\n",
    "    def create_tokenizer(self) -> \"ChronosTokenizer\":\n",
    "        class_ = getattr(chronos, self.tokenizer_class)\n",
    "        return class_(**self.tokenizer_kwargs, config=self)\n",
    "\n",
    "\n",
    "class ChronosTokenizer:\n",
    "    \"\"\"\n",
    "    A ``ChronosTokenizer`` definines how time series are mapped into token IDs\n",
    "    and back.\n",
    "\n",
    "    For details, see the ``input_transform`` and ``output_transform`` methods,\n",
    "    which concrete classes must implement.\n",
    "    \"\"\"\n",
    "\n",
    "    def input_transform(\n",
    "        self, context: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Any]:\n",
    "        \"\"\"\n",
    "        Turn a batch of time series into token IDs, attention map, and scale.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        context\n",
    "            A tensor shaped (batch_size, time_length), containing the\n",
    "            timeseries to forecast. Use left-padding with ``torch.nan``\n",
    "            to align time series of different lengths.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        token_ids\n",
    "            A tensor of integers, shaped (batch_size, time_length + 1)\n",
    "            if ``config.use_eos_token`` and (batch_size, time_length)\n",
    "            otherwise, containing token IDs for the input series.\n",
    "        attention_mask\n",
    "            A boolean tensor, same shape as ``token_ids``, indicating\n",
    "            which input observations are not ``torch.nan`` (i.e. not\n",
    "            missing nor padding).\n",
    "        tokenizer_state\n",
    "            An object that will be passed to ``output_transform``.\n",
    "            Contains the relevant context to decode output samples into\n",
    "            real values, such as location and scale parameters.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_transform(\n",
    "        self, samples: torch.Tensor, tokenizer_state: Any\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Turn a batch of sample token IDs into real values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples\n",
    "            A tensor of integers, shaped (batch_size, num_samples, time_length),\n",
    "            containing token IDs of sample trajectories.\n",
    "        tokenizer_state\n",
    "            An object returned by ``input_transform`` containing\n",
    "            relevant context to decode samples, such as location and scale.\n",
    "            The nature of this depends on the specific tokenizer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        forecasts\n",
    "            A real tensor, shaped (batch_size, num_samples, time_length),\n",
    "            containing forecasted sample paths.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MeanScaleUniformBins(ChronosTokenizer):\n",
    "    def __init__(\n",
    "        self, low_limit: float, high_limit: float, config: ChronosConfig\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.centers = torch.linspace(\n",
    "            low_limit,\n",
    "            high_limit,\n",
    "            config.n_tokens - config.n_special_tokens - 1,\n",
    "        )\n",
    "        self.boundaries = torch.concat(\n",
    "            (\n",
    "                torch.tensor([-1e20], device=self.centers.device),\n",
    "                (self.centers[1:] + self.centers[:-1]) / 2,\n",
    "                torch.tensor([1e20], device=self.centers.device),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def input_transform(\n",
    "        self, context: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        batch_size, length = context.shape\n",
    "\n",
    "        if length > self.config.context_length:\n",
    "            context = context[..., -self.config.context_length :]\n",
    "\n",
    "        attention_mask = ~torch.isnan(context)\n",
    "        scale = torch.nansum(\n",
    "            torch.abs(context) * attention_mask, dim=-1\n",
    "        ) / torch.nansum(attention_mask, dim=-1)\n",
    "        scale[~(scale > 0)] = 1.0\n",
    "        scaled_context = context / scale.unsqueeze(dim=-1)\n",
    "        token_ids = (\n",
    "            torch.bucketize(\n",
    "                input=scaled_context,\n",
    "                boundaries=self.boundaries,\n",
    "                # buckets are open to the right, see:\n",
    "                # https://pytorch.org/docs/2.1/generated/torch.bucketize.html#torch-bucketize\n",
    "                right=True,\n",
    "            )\n",
    "            + self.config.n_special_tokens\n",
    "        )\n",
    "        token_ids[~attention_mask] = self.config.pad_token_id\n",
    "\n",
    "        if self.config.use_eos_token:\n",
    "            eos_tokens = torch.full(\n",
    "                (batch_size, 1), fill_value=self.config.eos_token_id\n",
    "            )\n",
    "            token_ids = torch.concat((token_ids, eos_tokens), dim=1)\n",
    "            eos_mask = torch.full((batch_size, 1), fill_value=True)\n",
    "            attention_mask = torch.concat((attention_mask, eos_mask), dim=1)\n",
    "\n",
    "        return token_ids, attention_mask, scale\n",
    "\n",
    "    def output_transform(\n",
    "        self, samples: torch.Tensor, scale: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        scale_unsqueezed = scale.unsqueeze(-1).unsqueeze(-1)\n",
    "        indices = torch.clamp(\n",
    "            samples - self.config.n_special_tokens,\n",
    "            min=0,\n",
    "            max=len(self.centers) - 1,\n",
    "        )\n",
    "        return self.centers[indices] * scale_unsqueezed\n",
    "\n",
    "\n",
    "class ChronosModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``\n",
    "    and uses it to predict sample paths for time series tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config\n",
    "        The configuration to use.\n",
    "    model\n",
    "        The pre-trained model to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: ChronosConfig, model: PreTrainedModel) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.model.device\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extract the encoder embedding for the given token sequences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids\n",
    "            Tensor of indices of input sequence tokens in the vocabulary\n",
    "            with shape (batch_size, sequence_length).\n",
    "        attention_mask\n",
    "            A mask tensor of the same shape as input_ids to avoid attending\n",
    "            on padding or missing tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding\n",
    "            A tensor of encoder embeddings with shape\n",
    "            (batch_size, sequence_length, d_model).\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self.config.model_type == \"seq2seq\"\n",
    "        ), \"Encoder embeddings are only supported for encoder-decoder models\"\n",
    "        return self.model.encoder(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).last_hidden_state\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        prediction_length: Optional[int] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict future sample tokens for the given token sequences.\n",
    "\n",
    "        Arguments ``prediction_length``, ``num_samples``, ``temperature``,\n",
    "        ``top_k``, ``top_p`` can be used to customize the model inference,\n",
    "        and default to the corresponding attributes in ``self.config`` if\n",
    "        not provided.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples\n",
    "            A tensor of integers, shaped (batch_size, num_samples, time_length),\n",
    "            containing forecasted sample paths.\n",
    "        \"\"\"\n",
    "        if prediction_length is None:\n",
    "            prediction_length = self.config.prediction_length\n",
    "        if num_samples is None:\n",
    "            num_samples = self.config.num_samples\n",
    "        if temperature is None:\n",
    "            temperature = self.config.temperature\n",
    "        if top_k is None:\n",
    "            top_k = self.config.top_k\n",
    "        if top_p is None:\n",
    "            top_p = self.config.top_p\n",
    "\n",
    "        preds = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            generation_config=GenerationConfig(\n",
    "                min_new_tokens=prediction_length,\n",
    "                max_new_tokens=prediction_length,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=num_samples,\n",
    "                eos_token_id=self.config.eos_token_id,\n",
    "                pad_token_id=self.config.pad_token_id,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.config.model_type == \"seq2seq\":\n",
    "            preds = preds[..., 1:]  # remove the decoder start token\n",
    "        else:\n",
    "            assert self.config.model_type == \"causal\"\n",
    "            assert preds.size(-1) == input_ids.size(-1) + prediction_length\n",
    "            preds = preds[..., -prediction_length:]\n",
    "\n",
    "        return preds.reshape(input_ids.size(0), num_samples, -1)\n",
    "\n",
    "\n",
    "def left_pad_and_stack_1D(tensors: List[torch.Tensor]):\n",
    "    max_len = max(len(c) for c in tensors)\n",
    "    padded = []\n",
    "    for c in tensors:\n",
    "        assert isinstance(c, torch.Tensor)\n",
    "        assert c.ndim == 1\n",
    "        padding = torch.full(\n",
    "            size=(max_len - len(c),), fill_value=torch.nan, device=c.device\n",
    "        )\n",
    "        padded.append(torch.concat((padding, c), dim=-1))\n",
    "    return torch.stack(padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077b18fe-5f8f-4422-b0e3-3baca2fa983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chronos\n",
    "class ChronosPipeline:\n",
    "    \"\"\"\n",
    "    A ``ChronosPipeline`` uses the given tokenizer and model to forecast\n",
    "    input time series.\n",
    "\n",
    "    Use the ``from_pretrained`` class method to load serialized models.\n",
    "    Use the ``predict`` method to get forecasts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer\n",
    "        The tokenizer object to use.\n",
    "    model\n",
    "        The model to use.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: ChronosTokenizer\n",
    "    model: ChronosModel\n",
    "\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def _prepare_and_validate_context(\n",
    "        self, context: Union[torch.Tensor, List[torch.Tensor]]\n",
    "    ):\n",
    "        if isinstance(context, list):\n",
    "            context = left_pad_and_stack_1D(context)\n",
    "        assert isinstance(context, torch.Tensor)\n",
    "        if context.ndim == 1:\n",
    "            context = context.unsqueeze(0)\n",
    "        assert context.ndim == 2\n",
    "\n",
    "        return context\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed(\n",
    "        self, context: Union[torch.Tensor, List[torch.Tensor]]\n",
    "    ) -> Tuple[torch.Tensor, Any]:\n",
    "        \"\"\"\n",
    "        Get encoder embeddings for the given time series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        context\n",
    "            Input series. This is either a 1D tensor, or a list\n",
    "            of 1D tensors, or a 2D tensor whose first dimension\n",
    "            is batch. In the latter case, use left-padding with\n",
    "            ``torch.nan`` to align series of different lengths.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings, tokenizer_state\n",
    "            A tuple of two tensors: the encoder embeddings and the tokenizer_state,\n",
    "            e.g., the scale of the time series in the case of mean scaling.\n",
    "            The encoder embeddings are shaped (batch_size, context_length, d_model)\n",
    "            or (batch_size, context_length + 1, d_model), where context_length\n",
    "            is the size of the context along the time axis if a 2D tensor was provided\n",
    "            or the length of the longest time series, if a list of 1D tensors was\n",
    "            provided, and the extra 1 is for EOS.\n",
    "        \"\"\"\n",
    "        context_tensor = self._prepare_and_validate_context(context=context)\n",
    "        token_ids, attention_mask, tokenizer_state = self.tokenizer.input_transform(\n",
    "            context_tensor\n",
    "        )\n",
    "        embeddings = self.model.encode(\n",
    "            input_ids=token_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask.to(self.model.device),\n",
    "        ).cpu()\n",
    "        return embeddings, tokenizer_state\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        context: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        prediction_length: Optional[int] = None,\n",
    "        num_samples: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        limit_prediction_length: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get forecasts for the given time series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        context\n",
    "            Input series. This is either a 1D tensor, or a list\n",
    "            of 1D tensors, or a 2D tensor whose first dimension\n",
    "            is batch. In the latter case, use left-padding with\n",
    "            ``torch.nan`` to align series of different lengths.\n",
    "        prediction_length\n",
    "            Time steps to predict. Defaults to what specified\n",
    "            in ``self.model.config``.\n",
    "        num_samples\n",
    "            Number of sample paths to predict. Defaults to what\n",
    "            specified in ``self.model.config``.\n",
    "        temperature\n",
    "            Temperature to use for generating sample tokens.\n",
    "            Defaults to what specified in ``self.model.config``.\n",
    "        top_k\n",
    "            Top-k parameter to use for generating sample tokens.\n",
    "            Defaults to what specified in ``self.model.config``.\n",
    "        top_p\n",
    "            Top-p parameter to use for generating sample tokens.\n",
    "            Defaults to what specified in ``self.model.config``.\n",
    "        limit_prediction_length\n",
    "            Force prediction length smaller or equal than the\n",
    "            built-in prediction length from the model. True by\n",
    "            default. When true, fail loudly if longer predictions\n",
    "            are requested, otherwise longer predictions are allowed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        samples\n",
    "            Tensor of sample forecasts, of shape\n",
    "            (batch_size, num_samples, prediction_length).\n",
    "        \"\"\"\n",
    "        context_tensor = self._prepare_and_validate_context(context=context)\n",
    "\n",
    "        if prediction_length is None:\n",
    "            prediction_length = self.model.config.prediction_length\n",
    "\n",
    "        if prediction_length > self.model.config.prediction_length:\n",
    "            msg = (\n",
    "                f\"We recommend keeping prediction length <= {self.model.config.prediction_length}. \"\n",
    "                \"The quality of longer predictions may degrade since the model is not optimized for it. \"\n",
    "            )\n",
    "            if limit_prediction_length:\n",
    "                msg += \"You can turn off this check by setting `limit_prediction_length=False`.\"\n",
    "                raise ValueError(msg)\n",
    "            warnings.warn(msg)\n",
    "\n",
    "        predictions = []\n",
    "        remaining = prediction_length\n",
    "\n",
    "        while remaining > 0:\n",
    "            token_ids, attention_mask, scale = self.tokenizer.input_transform(\n",
    "                context_tensor\n",
    "            )\n",
    "            samples = self.model(\n",
    "                token_ids.to(self.model.device),\n",
    "                attention_mask.to(self.model.device),\n",
    "                min(remaining, self.model.config.prediction_length),\n",
    "                num_samples,\n",
    "                temperature,\n",
    "                top_k,\n",
    "                top_p,\n",
    "            )\n",
    "            prediction = self.tokenizer.output_transform(\n",
    "                samples.to(scale.device), scale\n",
    "            )\n",
    "\n",
    "            predictions.append(prediction)\n",
    "            remaining -= prediction.shape[-1]\n",
    "\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "            context_tensor = torch.cat(\n",
    "                [context_tensor, prediction.median(dim=1).values], dim=-1\n",
    "            )\n",
    "\n",
    "        return torch.cat(predictions, dim=-1)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Load the model, either from a local path or from the HuggingFace Hub.\n",
    "        Supports the same arguments as ``AutoConfig`` and ``AutoModel``\n",
    "        from ``transformers``.\n",
    "        \"\"\"\n",
    "\n",
    "        config = AutoConfig.from_pretrained(*args, **kwargs)\n",
    "\n",
    "        assert hasattr(config, \"chronos_config\"), \"Not a Chronos config file\"\n",
    "\n",
    "        chronos_config = ChronosConfig(**config.chronos_config)\n",
    "\n",
    "        if chronos_config.model_type == \"seq2seq\":\n",
    "            inner_model = AutoModelForSeq2SeqLM.from_pretrained(*args, **kwargs)\n",
    "        else:\n",
    "            assert config.model_type == \"causal\"\n",
    "            inner_model = AutoModelForCausalLM.from_pretrained(*args, **kwargs)\n",
    "\n",
    "        return cls(\n",
    "            tokenizer=chronos_config.create_tokenizer(),\n",
    "            model=ChronosModel(config=chronos_config, model=inner_model),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de35767-4552-4081-91f7-020199286d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chronos import ChronosPipeline\n",
    "import torch\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cpu\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model=pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e98dbbb5-d867-4735-86dd-6b049d3119b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([280.2986])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n",
    "\n",
    "context = torch.tensor(df[\"#Passengers\"])\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\",  # use \"cpu\" for CPU inference and \"mps\" for Apple Silicon\n",
    "    torch_dtype=torch.bfloat16)\n",
    "\n",
    "chronos_config = ChronosConfig(**config.chronos_config)\n",
    "tokenizer=chronos_config.create_tokenizer()\n",
    "\n",
    "context_tensor = pipeline._prepare_and_validate_context(context)\n",
    "token_ids, attention_mask, scale = tokenizer.input_transform(context_tensor)\n",
    "scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "277171bc-ba25-4411-baf0-41735b835ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, sequence_length, prediction_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            series (Tensor): A tensor containing the time series data.\n",
    "            sequence_length (int): The number of time steps to use as input.\n",
    "            prediction_length (int): The number of time steps to predict.\n",
    "        \"\"\"\n",
    "        self.series = series\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series) - self.sequence_length - self.prediction_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx is the starting point, we slice the sequence length for input\n",
    "        and slice the prediction length for target output\n",
    "        '''\n",
    "        x = self.series[idx:idx + self.sequence_length]\n",
    "        y = self.series[idx + self.sequence_length:idx + self.sequence_length + self.prediction_length]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "sequence_length = 10  # Length of the input sequence\n",
    "prediction_length = 5  # Length of the output sequence you want to predict\n",
    "# Convert data array to a torch tensor\n",
    "time_series_tensor = torch.tensor(df[\"#Passengers\"], dtype=torch.float32)  \n",
    "\n",
    "dataset = UnivariateTimeSeriesDataset(time_series_tensor, sequence_length, prediction_length)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "train_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14cba0b0-e4d6-4e7b-b53f-75f6ada59a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1704.2322998046875\n",
      "Epoch 2: Loss = 1140.099853515625\n",
      "Epoch 3: Loss = 218.08677673339844\n",
      "Epoch 4: Loss = 390.4356689453125\n",
      "Epoch 5: Loss = 12134.8427734375\n",
      "Epoch 6: Loss = 983.6668701171875\n",
      "Epoch 7: Loss = 5864.1484375\n",
      "Epoch 8: Loss = 428.2220764160156\n",
      "Epoch 9: Loss = 632.1346435546875\n",
      "Epoch 10: Loss = 1477.0731201171875\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Enable trainable parameters\n",
    "model.train()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        token_ids, attention_mask, scale = tokenizer.input_transform(inputs)\n",
    "\n",
    "        samples = model(\n",
    "                token_ids,\n",
    "                attention_mask,\n",
    "                prediction_length,\n",
    "                num_samples=30,\n",
    "                temperature=1.0,\n",
    "                top_k=50,\n",
    "                top_p=1.0,\n",
    "            )\n",
    "\n",
    "        predictions = []\n",
    "        remaining = prediction_length\n",
    "\n",
    "        while remaining > 0:\n",
    "            token_ids, attention_mask, scale = tokenizer.input_transform(inputs)\n",
    "\n",
    "            prediction = tokenizer.output_transform(\n",
    "                samples, scale\n",
    "            )\n",
    "\n",
    "            predictions.append(prediction.median(dim=1))\n",
    "            remaining -= prediction.shape[-1]\n",
    "\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "        preds = predictions[0].values.median(dim=1).values\n",
    "        actuals = targets.median(dim=1).values\n",
    "        loss = criterion(preds, actuals)\n",
    "        if not loss.requires_grad:\n",
    "            loss = loss.clone().requires_grad_(True)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: Loss = {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a54b784-07c2-47b2-91e3-22f14be6bb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ccb0a-e636-4a80-b097-ddaa67f7f59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7f62b-de41-4a38-97ba-7617915f2018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
